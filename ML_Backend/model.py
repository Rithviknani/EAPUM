# -*- coding: utf-8 -*-
"""AllClassifiers3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Svarpy/EAPUM/blob/main/AllClassifiers3.ipynb
"""

import numpy as np
import pandas as pd
pd.set_option('max_columns', None)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC, SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import os

#url = 'https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset'
#data = pd.read_csv(url)
#from google.colab import files
#uploaded = files.upload()
#path = '/'
#trainer_path = os.path.join('emp_train.csv')
#data = pd.read_csv(trainer_path)
data = pd.read_csv('C:\\Users\\Yashashchandra Kollu\\Desktop\\eapum\\model\\emp_train.csv')

#data

#data.info()

##print(data.shape)
#{col: len(data[col].unique()) for col in data.columns}

def tr2(df):
  df = df.copy()
  for col in df.columns:
    if(df[col].dtype == np.number):
      continue
    df[col] = LabelEncoder().fit_transform(df[col])
  return df

def preprocess_inputs(df):
    df = df.copy()
    
    # Drop single-value columns and id columns
    df = df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=1)

    # Binary-encode binary columns
    df['Gender'] = df['Gender'].replace({'Female': 0, 'Male': 1})
    df['OverTime'] = df['OverTime'].replace({'No': 0, 'Yes': 1})
      
    # Ordinal-encode the BusinessTravel column
    df['BusinessTravel'] = df['BusinessTravel'].replace({'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2})
    
    # One-hot encoding
    #for column in ['Department', 'EducationField', 'JobRole', 'MaritalStatus']:
        #df = onehot_encode(df, column=column)
    
    df = tr2(df)
    
    # Split df into X and y
    y = df['Attrition']
    X = df.drop('Attrition', axis=1)
    
    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)
    
    # Scale X
    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)
    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)
    
    return X_train, X_test, y_train, y_test,scaler

def op_preprocess(sample,scaler):
    df = sample.copy()
    
    # Drop single-value columns and id columns
    df = df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=1)

    # Binary-encode binary columns
    df['Gender'] = df['Gender'].replace({'Female': 0, 'Male': 1})
    df['OverTime'] = df['OverTime'].replace({'No': 0, 'Yes': 1})
      
    # Ordinal-encode the BusinessTravel column
    df['BusinessTravel'] = df['BusinessTravel'].replace({'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2})
    
    # One-Hot encoding
    #for column in ['Department', 'EducationField', 'JobRole', 'MaritalStatus']:
        #df = onehot_encode(df, column=column)

    df = tr2(df)
    
    ##print("Inside==============================================")
    ##print(df.shape)
    
    # Split df into X and y
    #y = df['Attrition']
    #X = df.drop('Attrition', axis=1)
    
    # Train-test split
    #X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)
    
    # Scale X
    #scaler = StandardScaler()
    #scaler.fit(X_train)
    #X_test = pd.DataFrame(scaler.transform(df), index=df.index, columns=df.columns)
    #X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)

    df = pd.DataFrame(scaler.transform(df), index=df.index, columns=df.columns)
    
    return df

X_train, X_test, y_train, y_test,scaler = preprocess_inputs(data)

#print(X_train)

#print(y_train)

"""**Training**"""

models = {
    "                   Logistic Regression": LogisticRegression(),
    "                   K-Nearest Neighbors": KNeighborsClassifier(),
    "                         Decision Tree": DecisionTreeClassifier(),
    "Support Vector Machine (Linear Kernel)": LinearSVC(max_iter=1500),
    "   Support Vector Machine (RBF Kernel)": SVC(),
    "                        Neural Network": MLPClassifier(max_iter=1500),
    "                         Random Forest": RandomForestClassifier(),
    "                     Gradient Boosting": GradientBoostingClassifier()
}

#pre_models = models.copy()

#print("Normalized Dataset")
'''
for name, model in models.items():
    model.fit(X_train, y_train)
    ##print(name + " trained.")
'''
#print('--------------------------------')
'''
#print("Handpicked Dataset")
for name, pre_model in pre_models.items():
    pre_model.fit(pre_X_train, pre_y_train)
    ##print(name + " trained.")'''

"""Output"""

#print("Normalized dataset")
#for name, model in models.items():
#print(name + ": {:.2f}%".format(model.score(X_test, y_test) * 100))
#print("------------------------------")


ea_svm = LinearSVC(max_iter=1500)

ea_svm.fit(X_train,y_train)

'''
#Predicting part, etc..
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
pred = ea_svm.predict(X_test)

cm = confusion_matrix(y_test,pred)
#print(cm)

#print("Accuracy:", accuracy_score(y_test,pred))
#print(classification_report(y_test,pred))

df = data.iloc[0:2,:]
#print(df['Attrition'])
df = df.drop('Attrition', axis = 1)

##print("Dataframe\n",df)
#df = np.array(df)

##print(df.shape)

#smple = pd.DataFrame(df,columns=data.columns)
#smple

#smple.drop('Attrition',axis = 1)

##print(smple.shape)

#df = op_preprocess(df,scaler)

##print("Dataframe\n",df)

##print(df.shape)

#model["Support Vector Machine (Linear Kernel)"].predict(smple)

df = pd.read_csv('emp_test.csv')
df = op_preprocess(df,scaler)
pred = ea_svm.predict(df)
pred


#df = data.iloc[0:2,:]
df = pd.read_csv('emp_test.csv')
##print(df['Attrition'])
#df = df.drop('Attrition', axis = 1)
df = op_preprocess(df,scaler)
for row in range(df.shape[0]):
  for (intercept, coef) in zip(ea_svm.intercept_, ea_svm.coef_):
      s = "y = {0:.3f}".format(intercept)
      h = intercept
      mx = -1
      for (i, c) in enumerate(coef):
          s += " + {0:.3f} * x{1}".format(c, i)
          k = max(h,c*df.iloc[row][i])
          if(k>h):
            h = k
            mx = i
      ##print(s)
      #print(h,mx)
      #print(pred[row],df.columns[mx])'''


import pickle #For storing the model
# open a file, where you ant to store the data
file = open('svm.pkl', 'wb')

# dump information to that file
pickle.dump(ea_svm, file)
